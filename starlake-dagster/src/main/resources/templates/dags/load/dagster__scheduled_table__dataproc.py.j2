# This template executes individual dataproc jobs and requires the following dag generation options set:
# - dataproc_name: the name of the dataproc cluster [OPTIONAL]
# - dataproc_project_id: the project id of the dataproc cluster (if not set, the project id of the composer environment will be used) [OPTIONAL]
# - dataproc_region(europe-west1): the region of the dataproc cluster (if not set, europe-west1 will be used) [OPTIONAL]
# - dataproc_subnet(default): the subnetwork of the dataproc cluster (if not set, the default subnetwork will be used) [OPTIONAL]
# - dataproc_service_account: the service account of the dataproc cluster (if not set, the default service account will be used) [OPTIONAL]
# - dataproc_image_version(2.2-debian12): the image version of the dataproc cluster (if not set, 2.2-debian12 will be used) [OPTIONAL]
# - dataproc_master_machine_type(n1-standard-4): the master machine type of the dataproc cluster (if not set, n1-standard-4 will be used) [OPTIONAL]
# - dataproc_master_disk_size(1024): the master disk size of the dataproc cluster (if not set, 1024 will be used) [OPTIONAL]
# - dataproc_master_disk_type(pd-standard): the master disk type of the dataproc cluster (if not set, pd-standard will be used) [OPTIONAL]
# - dataproc_worker_machine_type(n1-standard-4): the worker machine type of the dataproc cluster (if not set, n1-standard-4 will be used) [OPTIONAL]
# - dataproc_worker_disk_size(1024): the worker disk size of the dataproc cluster (if not set, 1024 will be used) [OPTIONAL]
# - dataproc_worker_disk_type(pd-standard): the worker disk type of the dataproc cluster (if not set, pd-standard will be used) [OPTIONAL]
# - dataproc_num_workers(4): the number of workers of the dataproc cluster (if not set, 4 will be used) [OPTIONAL]
# - dataproc_cluster_metadata: the metadata to add to the dataproc cluster specified as a map in json format [OPTIONAL]
# - spark_jar_list: the list of spark jars to be used [REQUIRED]
# - spark_bucket: the bucket to use for spark and biqquery temporary storage [REQUIRED]
# - spark_job_main_class(ai.starlake.job.Main): the main class of the spark job (if not set, the main class ai.starlake.job.Main will be used) [OPTIONAL]
# - spark_executor_memory(11g): the amount of memory to use per executor process (if not set, 11g will be used) [OPTIONAL]
# - spark_executor_cores(4): the number of cores to use on each executor (if not set, 4 will be used) [OPTIONAL]
# - spark_executor_instances(1): the number of executor instances (if not set, 1 will be used) [OPTIONAL]
# - sl_env_var: starlake variables specified as a map in json format - at least the root project path SL_ROOT should be specified [OPTIONAL]
# - pre_load_strategy(none): The optional pre-load strategy to use to conditionaly load a domain, one of imported, ack, pending or none (if not set, the default 'none' strategy will be used) [OPTIONAL]
# - global_ack_file_path: when the domain preloading strategy has been set to 'ack', the path to the global ack file [OPTIONAL]
# - ack_wait_timeout(3600): when the domain preloading strategy has been set to 'ack', the timeout in seconds to wait for the ack file [OPTIONAL]
# - tags: a list of tags to be applied to the dag [OPTIONAL]
# - retries(1): the number of retries to attempt before failing the task [OPTIONAL]
# - retry_delay(300): the delay between retries in seconds [OPTIONAL]
# Naming rule: scheduled or sensor, global or domain or table, cloudrun or bash or dataproc or serverless with free-text
{% include 'templates/dags/__starlake_dagster_orchestrator.py' %}
{% include 'templates/dags/__starlake_dataproc_execution.py' %}
{% include 'templates/dags/load/__scheduled_table_tpl.py.j2' %}
